<!DOCTYPE html>




<html class="theme-next mist" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="n86IUFnBFOxfeqHBuzwyDN9z4IxNVMohQEh6b6rUtYo" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Hoey笔记">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hoey笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Hoey">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Hoey笔记</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?43f2ad51dd03ad146d67cc622d4e11e2";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




<meta name="generator" content="Hexo 5.4.2"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hoey笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">和有趣的人做尽有趣的事</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a target="_blank" href="http://hoey.tk" rel="section noopener">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>

<style>
  #VOCECHAT_WIDGET {
    bottom: 85px !important;
    right: 25px !important;
  }
</style>

<script
  data-host-id="1"
  data-auto-reg="true"
  data-login-token=""
  data-close-width="35"
  data-close-height="35"
  data-open-width="380"
  data-open-height="680"
  data-position="right"
  data-welcome="请问有什么事？"
  src="https://talk.zyh1.cc/widget.js"
  async
></script>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250721093711-32862603.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250721093711-32862603.html" itemprop="url">科普各个出海线路</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-07-21T09:37:11+08:00">
                2025-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><p><strong>整体的线路大概排名:<code>AS23764(CTGNet)</code>≈<code>AS4809(CN2 GIA)</code>&gt;<code>AS9929(CNCNet)</code>&gt;<code>AS9808(CMI)</code>&gt;<code>AS4837(CUVIP)</code>&gt;<code>AS4837(169)</code>&gt;<code>AS4809(CN2 GT)</code>≈<code>AS4134(CN163)</code></strong></p>
<p>判断方法：</p>
<table>
<thead>
<tr>
<th align="left">网络</th>
<th align="left">IP地址</th>
</tr>
</thead>
<tbody><tr>
<td align="left">CN2 GIA</td>
<td align="left">59.43.*.*（全部）</td>
</tr>
<tr>
<td align="left">CN2 GT</td>
<td align="left">59.43.*.<em>（国外）/ 202.97.</em>.*（国内）</td>
</tr>
<tr>
<td align="left">163</td>
<td align="left">202.97.*.*（全部）</td>
</tr>
<tr>
<td align="left">AS9929</td>
<td align="left">218.105.<em>.</em> / 210.51.<em>.</em></td>
</tr>
<tr>
<td align="left">CUVIP</td>
<td align="left">同AS4837，但走美西(SJC)出口</td>
</tr>
<tr>
<td align="left">AS4837</td>
<td align="left">联通默认</td>
</tr>
</tbody></table>
<h2 id="电信"><a href="#电信" class="headerlink" title="电信"></a>电信</h2><p>电信：<code>CTGNet</code>≈<code>CN2 GIA</code>&gt;<code>CN2 GT</code>&gt;<code>163直连</code>。</p>
<h3 id="ChinaNet-163网"><a href="#ChinaNet-163网" class="headerlink" title="ChinaNet(163网)"></a>ChinaNet(163网)</h3><p>ChinaNet(也就是163网)，主要城市如LAX、SJC、FRA、LON，回国带宽大，但因电信超卖严重，因此尖峰时段较容易阻塞，同时也常有来自中国境内的流量攻击将出口塞满。价格较低，也是绝大部分普通家宽的默认出境路线。</p>
<h3 id="CN2-GT-Global-Transit-CN2-GT（全球过境）"><a href="#CN2-GT-Global-Transit-CN2-GT（全球过境）" class="headerlink" title="CN2 GT (Global Transit) CN2 GT（全球过境）"></a>CN2 GT (Global Transit) CN2 GT（全球过境）</h3><p>CN2 GT，163与CN2 POP之间统一在上海、广州、北京三个互连点，北京互连点为新增节点。同上一点所属，由于两张网流量交换前也会经过163的汇聚点，因此也较容易因163汇聚点有大量攻击流量导致阻塞，同时因攻击塞满两张网之间的互连也时常发生。</p>
<p>对于跨境互联质量，电信官网承认CN2 GT与ChinaNet质量无异。换句话说就是，CN2 GT集成了电信两张网各自最烂的部分：CN2海外的憨批骨干质量和163出口的傻逼跨境段质量。</p>
<p>欧美地区的CN2 GT或将在接下来规划下线，现有CN2 GT的客户（已知的包括QuadraNet AS8100, 俄罗斯DataLine等）将被切换至163。但正如前文所述，在跨境质量上CN2 GT和163因为是共享C-I段，因此在质量上并不会有差别或者变化。</p>
<h3 id="CN2-GIA-Global-Internet-Access"><a href="#CN2-GIA-Global-Internet-Access" class="headerlink" title="CN2 GIA (Global Internet Access)"></a>CN2 GIA (Global Internet Access)</h3><p>GIA与GT之间所经过的跨境互联段不同（即C-I段），其中GIA有单独的C-I段容量，而GT则与163共享C-I段，因此基于目前的跨境互联质量而言，CN2 GT与163几乎没有差异，但GIA与GT/163则会有明显体验上的差异。该产品在163与CN2两张网之间互连有独立的端口进行互连，互连点也较多，采就近接入原则，但实际上延迟不一定比较好。且因互连点不同关系，实际上有可能会出现某地至北美GT走上海出口，但GIA却走广东出口等等差异。另互连端口在有大量攻击时容易阻塞。特别是南京、北京互连点。</p>
<p>电信报价依据所在地与中国大陆之间的延迟决定，延迟越高价格越低，如北美大约6 USD/Mbps (目前可能已涨价），而香港地区的CN2 GIA单价最高则达到了100 USD/Mbps，底线价格也有接近$80（但应该很少能有以这个价格拉到的）。<strong>2022年3月份</strong>左右CN2的跨境段容量峰值使用率已超过90%+，电信集团目前已经暂停了部分方向新接的CN2 GIA的订单（除非有释放的带宽资源，释放多少批多少）。</p>
<p><strong>与AS4809直接互联的CN2 GIA产品在亚太和欧洲地区已经下线，目前新接客户需要通过CTGNet。</strong></p>
<h3 id="CTGNet-AS23764"><a href="#CTGNet-AS23764" class="headerlink" title="CTGNet (AS23764)"></a>CTGNet (AS23764)</h3><p>由于通过CTG/CTA/CTE购买GIA带宽需要中国电信集团级别审批合约，因此CTG自己搞了个CTGNet（自卖自销系列），据大佬介绍CTGNet订购的带宽合约目前暂不需要集团审批。在今年年底某个时间点以后新开的合约都需要走CTGNet签订。目前电信正在将已有的AS4809客户迁移到CTGNet，亚太地区比如香港和新加坡已经迁移不少客户，包括阿里云香港、腾讯等，今后新开的大部分CN2合同都会变成CTGNet的合同。</p>
<p>CTGNet包含了之前的163/CN2 GT和CN2 GIA的接入，区别是根据合约报价来决定最终互联的质量，钱给的多就给你GIA，钱给的少那就是GT甚至163。</p>
<p>目前亚太和欧洲（除俄罗斯以外），原有的CN2 AS4809的客户大部分已经迁移至CTGNet下。</p>
<p>CTGNet常见的骨干网IP开头为 69.194 和 203.22，均在AS23764下。</p>
<p><img src="https://blog.sunflyer.cn/wp-content/uploads/2022/04/image-1-1024x580.png" alt="img"></p>
<h2 id="联通"><a href="#联通" class="headerlink" title="联通"></a>联通</h2><p>联通：<code>三网回程GIA</code> &gt; <code>三网回程9929</code> &gt; <code>三网回程4837</code>。</p>
<p>中国联通目前有3个主要ASN：AS4837（中国联通骨干网），AS10099（中国联通国际，CUG）以及AS9929（中国联通工业互联网，CUII，以前的老网通的骨干，简称A网）</p>
<p>一般家宽客户接入的是4837网络，与电信4134类似。国际出口分为北京、上海、广州三部分。比较常见的路由如北京出口承载多数省份的跨境流量，包含日韩北美欧洲俄罗斯等地区；上海出口承载至东亚/东南亚以及北美圣何塞的跨境流量；广州出口承载至港澳/东南亚/北美（洛杉矶）/欧洲部分地区的流量。</p>
<p>联通对其上游宣告的IP段有一定倾向性调整（这实际上也是流量工程和QoS的一部分），例如，对于部分IP段，联通在日本发给NTT的路由prepend了AS PATH，造成的情况就是对于同时接了NTT和其他网络的服务商（比如甲骨文东京接了NTT和TATA），由于AS PATH长度问题，至联通部分IP段（例如153.3.16.1）是NTT经由上海/北京互联回程，而部分IP段（例如 27.11.128.1） 则被送去了其他网络，出现绕美/高延迟的情况。这一点在联通至新加坡Cogent的互联也能看出来（部分段走新加坡的Cogent直接回到联通网内，部分则绕欧至联通与Cogent的欧洲互联）。</p>
<p>10099（CUG）与电信目前的CTGNet类似，提供至大陆方向的差异化接入，包括CUG（10099-&gt;4837）和CUG VIP（10099-&gt;9929-&gt;4837）</p>
<p>9929为老网通骨干网，现在的CUII，其本身负载低，所以被用于为政企提供高质量网络访问，但是缺点在于。。。这张网的跨境互联端点对比4837较少，而且多数互联的容量并不大。。。现在作为联通对标CN2 GIA的产品销售。</p>
<p>普通4837的报价依据互联地区不同而不同，日本/新加坡可以做到$20/Mbps甚至更低的价格。（但还是贵而且和电信一样是Best Effort），香港比日本/新加坡略高一些，≤$25/Mbps，欧洲地区 ≤$5/Mbps。</p>
<p>香港 10099 CN Premium Route (官方名称，实际上就是9929）的报价一般≥ $55/Mbps。</p>
<h2 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h2><p>移动线路原本是全部走香港出口的，自2022年初开始，随着上海临港出口启用，在日韩购买了CMI IP Transit的ISP/IDC回到国内部分省份移动的流量会经由上海出口承载，延迟对比绕广东会有所降低。</p>
<p>目前的主要出口还是广州-香港段，IPv6跨境流量路由大部分会出现绕行上海的情况（即使你处在广东）。</p>
<p><em>注：移动线路的出口表现在不同省份不同线路的差异极大，对于广东、上海来说较好，对于内陆省份较差，需要根据实际情况测试决定</em></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="CTGNet-AS23764-CUG-AS10099-和-CMI-AS58453-三张网的差异性"><a href="#CTGNet-AS23764-CUG-AS10099-和-CMI-AS58453-三张网的差异性" class="headerlink" title="CTGNet (AS23764)/CUG (AS10099) 和 CMI (AS58453)三张网的差异性"></a>CTGNet (AS23764)/CUG (AS10099) 和 CMI (AS58453)三张网的差异性</h3><p>三家运营商目前都有趋势将国际客户的IPT业务转移至自身的国际网络下，但是三家的国际网络资源略有差异。</p>
<p>对于电信和联通，CTGNet和CUG提供的包括对客户的IPT接入以及差异化产品（例如普通IPT：CTGNet给到163，而10099给到4837；而对于精品IPT（Premium IP Transit），CTGNet给到的是原有的CN2 GIA产品，CUG给到的则是联通A网9929。这一点也可以从现有路由能看出来。此外，CTGNet和CUG也为两家运营商在海外的移动客户提供网络接入（例如CUniq和CTMO/CTexcel），并且两张网络也用于对应的国内CN2/9929精品网客户就近访问海外资源。CTGNet和CUG<strong>目前并不接管两家运营商全部的出海流量</strong>。</p>
<p>而对于移动，AS58453 CMI则是国内移动AS9808客户公网跨境的几乎唯一方向，并且移动的海外精品网访问接入是由AS58807来负责的，与电信CTGNet和联通CUG的角色上有较大的区别。</p>
<h3 id="境外VPS购买提示"><a href="#境外VPS购买提示" class="headerlink" title="境外VPS购买提示"></a>境外VPS购买提示</h3><p>电信用户如果不买优化线路（GIA/9929/4837/CMI回程/163优化线路等）用电信163回程晚高峰会很慢。</p>
<p>联通用户因为用户量少，出国线路日常就能满足，主流4837回程（推荐9929和软银/GIA/4837和香港CMI）。</p>
<p>移动用户主流CMI回程（推荐香港CMI/GIA和9929/普通线路4837）</p>
<p>整体位置速度：<code>香港</code>&gt;<code>韩国</code>&gt;<code>日本</code>&gt;<code>美国</code>。香港VPS比较贵。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250702154250-4dec7311.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250702154250-4dec7311.html" itemprop="url">跨模态应用检索设计图</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-07-02T15:42:50+08:00">
                2025-07-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Task: 任务样例：给定数据集，完成以下任务：基于给定数据集，完成图文数据对齐并生成Embedding向量；实现零样本分类，针对给定的10类未见标签图像输出分类准确率；可视化图文检索结果，并计算相似度矩阵等任务。</p>
<p><img src="https://pic.yihao.de/pic/2025/07/02/6864e302af1f5.png" alt="image-20250702154254512"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250625145404-1bc9cb81.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250625145404-1bc9cb81.html" itemprop="url">HuggingFace Embedding的模型提取</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-06-25T14:54:04+08:00">
                2025-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提取huggingface上关于embedding的模型 – 20250625</p>
<p>以下内容借助AI提取，仅做参考</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></td><td class="code"><pre><span class="line">[Qwen/Qwen3-Embedding-0.6B]: 解决问题：[将文本转换为向量表示问题] </span><br><span class="line">[Qwen/Qwen3-Embedding-8B]: 解决问题：[文本向量化及语义表征问题]</span><br><span class="line">[Qwen/Qwen3-Embedding-4B]: 解决问题：将文本转换为向量表示用于下游任务 </span><br><span class="line">[jinaai/jina-embeddings-v3]: 解决问题：文本向量化及语义相似度计算 </span><br><span class="line">Qwen/Qwen3-Embedding-0.6B-GGUF: 解决问题：将文本转换为向量表示 </span><br><span class="line">jinaai/jina-embeddings-v4: 解决问题：文本特征提取与语义表示 </span><br><span class="line">[Qwen/Qwen3-Embedding-8B-GGUF]: 解决问题：文本向量化以用于下游任务 </span><br><span class="line">[Qwen/Qwen3-Embedding-4B-GGUF]: 解决问题：文本特征提取与向量化表示 </span><br><span class="line">ai-sage/Giga-Embeddings-instruct: 解决问题：文本向量化以用于多种NLP任务 </span><br><span class="line">boboliu/Qwen3-Embedding-0.6B-W4A16-G128: 解决问题：文本向量化及语义表示问题 </span><br><span class="line">[pyannote/embedding]: 解决问题：语音特征嵌入表示相关问题 </span><br><span class="line">NeuML/pubmedbert-base-embeddings: 解决问题：处理生物医学文本特征表示</span><br><span class="line">desarrolloasesoreslocales/jina-embeddings-v2-base-en-finetuned-legalcorpus: 解决问题：处理英文法律文本嵌入问题 </span><br><span class="line">maidlun1020/bce-embedding-base_v1: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">[jinaai/jina-embeddings-v2-base-zh]: 解决问题：中文文本向量化表示问题 </span><br><span class="line">[jinaai/jina-embeddings-v2-base-de]: 解决问题：德语文本嵌入表示问题 </span><br><span class="line">Marqo/marqo-ecommerce-embeddings-L: 解决问题：电商领域文本特征嵌入问题 </span><br><span class="line">ibm-granite/granite-embedding-30m-english: 解决问题：英文文本向量化及语义表示问题 </span><br><span class="line">ibm-granite/granite-embedding-107m-multilingual: 解决问题：多语言文本嵌入表示问题 </span><br><span class="line">Salesforce/SFR-Embedding-Code-2B_R: 解决问题：代码相关的嵌入表示问题 </span><br><span class="line">alikia2x/jina-embedding-v3-m2v-256: 解决问题：文本到向量的嵌入表示问题 </span><br><span class="line">[AhmedZaky1/DIMI-embedding-sts-matryoshka]: 解决问题：语义文本相似度计算问题 </span><br><span class="line">onnx-community/Qwen3-Embedding-0.6B-ONNX: 解决问题：文本向量化及语义特征提取 </span><br><span class="line">electroglyph/Qwen3-Embedding-0.6B-onnx-uint8: 解决问题：将文本转换为向量表示 </span><br><span class="line">annamodels/LGAI-Embedding-Preview: 解决问题：未找到该模型: 解决问题相关信息 </span><br><span class="line">[C10X/Qwen3-Embedding-TurboX]: 解决问题：文本特征提取与语义表示问题 </span><br><span class="line">[janni-t/qwen3-embedding-0.6b-tei-onnx]: 解决问题：文本向量化及语义表示问题 </span><br><span class="line">itsbohara/qwen3_embedding_4b_q8_0: 解决问题：文本向量化及语义表示问题 </span><br><span class="line">MALIBA-AI/bambara-embeddings: 解决问题：生成班巴拉语的词嵌入表示 </span><br><span class="line">malcolmrey/embeddings: 解决问题：未找到该模型明确解决的问题信息 </span><br><span class="line">flax-sentence-embeddings/multi-qa_v1-distilbert-cls_dot: 解决问题：多语言问答文本嵌入表示问题 </span><br><span class="line">flax-sentence-embeddings/multi-qa_v1-distilbert-mean_cos: 解决问题：多语言问答任务语义匹配 </span><br><span class="line">flax-sentence-embeddings/multi-qa_v1-mpnet-cls_dot: 解决问题：多语言问答文本嵌入问题 </span><br><span class="line">flax-sentence-embeddings/multi-qa_v1-mpnet-mean_cos: 解决问题：多语言问答句嵌入表示问题 </span><br><span class="line">[flax-sentence-embeddings/reddit_single-context_mpnet-base]: 解决问题：处理Reddit文本单上下文嵌入问题 </span><br><span class="line">[flax-sentence-embeddings/st-codesearch-distilroberta-base]: 解决问题：[代码搜索相关语义匹配问题]</span><br><span class="line">flax-sentence-embeddings/stackoverflow_mpnet-base: 解决问题：处理Stack Overflow文本语义表示 </span><br><span class="line">[sentence-transformers/average_word_embeddings_glove.6B.300d]: 解决问题：将文本转换为词嵌入向量表示 </span><br><span class="line">sentence-transformers/average_word_embeddings_glove.840B.300d: 解决问题：将文本转换为固定长度向量表示 </span><br><span class="line">[sentence-transformers/average_word_embeddings_komninos]: 解决问题：生成句子嵌入用于语义表示 </span><br><span class="line">[sentence-transformers/average_word_embeddings_levy_dependency]: 解决问题：生成句子的平均词嵌入表示 </span><br><span class="line">[MrAnderson/bert-base-1024-full-trivia-copied-embeddings]: 解决问题：[处理问答类琐事相关任务] </span><br><span class="line">MrAnderson/nystrom-2048-full-trivia-copied-embeddings: 解决问题：处理琐事相关的嵌入表示问题 </span><br><span class="line">MrAnderson/yoso-2048-full-trivia-copied-embeddings: 解决问题：未找到相关信息，无法明确: 解决问题。 </span><br><span class="line">MrAnderson/yoso-512-full-trivia-copied-embeddings: 解决问题：处理问答琐事相关嵌入表示 </span><br><span class="line">MrAnderson/bert-base-2048-full-trivia-copied-embeddings: 解决问题：处理问答类琐事信息嵌入问题 </span><br><span class="line">MrAnderson/nystrom-1024-full-trivia-copied-embeddings: 解决问题：处理与琐事相关的嵌入表示问题 </span><br><span class="line">Kalaoke/embeddings_dense_model: 解决问题：暂未找到该模型: 解决问题的相关信息 </span><br><span class="line">MrAnderson/nystrom-4096-full-trivia-copied-embeddings: 解决问题：处理琐事相关的嵌入表示问题 </span><br><span class="line">MrAnderson/bert-base-4096-full-trivia-copied-embeddings: 解决问题：可能用于问答类知识相关嵌入表示 </span><br><span class="line">jwieting/simple-paraphrastic-embeddings: 解决问题：生成句子的释义嵌入表示 </span><br><span class="line">[shiemn/bigdatageo-gelectra-base-new-embeddings]: 解决问题：可能用于地理相关数据嵌入表示 </span><br><span class="line">[ITESM/sentece-embeddings-BETO]: 解决问题：生成西班牙语句子的嵌入表示 </span><br><span class="line">[espejelomar/sentece-embeddings-BETO]: 解决问题：生成西班牙语句子的嵌入表示 </span><br><span class="line">cambridgeltl/tweet-roberta-base-embeddings-v1: 解决问题：将推文转换为向量表示 </span><br><span class="line">sumedh/pretrained-word-embeddings: 解决问题：将单词转换为向量表示 </span><br><span class="line">florentgbelidji/blip_image_embeddings: 解决问题：将图像转换为向量嵌入表示 </span><br><span class="line">philschmid/all-MiniLM-L6-v2-optimum-embeddings: 解决问题：文本语义表示与相似度计算 </span><br><span class="line">crumb/ViT-L-14-Token-Embeddings: 解决问题：图像视觉特征的嵌入表示问题 </span><br><span class="line">ProGamerGov/knollingcase-embeddings-sd-v2-0: 解决问题：生成特定风格图像嵌入表示 </span><br><span class="line">mikumikugeek/embeddings: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">soknife/embeddings: 解决问题：暂未获取到该模型: 解决问题相关信息。 </span><br><span class="line">KnutJaegersberg/sentence_transformer_mpnet_v2_to_kg_embeddings_wikidata5m: 解决问题：将句子转换为知识图谱嵌入</span><br><span class="line">ProGamerGov/winter-cat-embeddings-sd-v2-1: 解决问题：用于StableDiffusion生成特定风格图像 </span><br><span class="line">rickysambora55/ai-embeddings: 解决问题：未找到该模型相关有效: 解决问题信息 </span><br><span class="line">SirVeggie/nixeu_embeddings: 解决问题：未在huggingface找到相关: 解决问题信息 </span><br><span class="line">Priyanshu9991/sd2.1_768_embeddings: 解决问题：生成StableDiffusion 2.1图像嵌入 </span><br><span class="line">anony12sub34/embeddings-lehmer-6-50-roberta_squad2.0: 解决问题：处理SQuAD 2.0问答任务相关问题 </span><br><span class="line">mikumikugeek/embeddings02: 解决问题：暂未找到该模型具体: 解决问题信息 </span><br><span class="line">anony12sub34/embeddings-hamming-6-100-roberta_squad2.0: 解决问题：适用于SQuAD 2.0问答任务</span><br><span class="line">[anony12sub34/embeddings-hamming-6-50-roberta_squad2.0]: 解决问题：[SQuAD 2.0问答任务嵌入问题] </span><br><span class="line">[ycool/embeddings-kenny-6-50-roberta_squad2.0]: 解决问题：[处理SQuAD 2.0问答任务相关问题]</span><br><span class="line">ycool/embeddings-kenny-6-100-roberta_squad2.0: 解决问题：用于SQuAD 2.0问答任务文本嵌入 </span><br><span class="line">tsukimiya/embeddings: 解决问题：未在huggingface找到相关: 解决问题信息 </span><br><span class="line">dranzerstar/SD-textual-inversion-embeddings-repo: 解决问题：用于Stable Diffusion文本反转嵌入</span><br><span class="line">EvaKlimentova/knots_M2_embeddings_alphafold: 解决问题：蛋白质结构中打结相关问题分析 </span><br><span class="line">ProGamerGov/knollingcase-embeddings-sd-v1-5: 解决问题：用于稳定扩散v1.5的嵌入相关问题 </span><br><span class="line">jjetho/embeddings: 解决问题：未在huggingface找到相关信息，无法确定。 </span><br><span class="line">garyw/clinical-embeddings-100d-w2v-cr: 解决问题：临床文本数据的嵌入表示问题 </span><br><span class="line">lckidwell/embeddings: 解决问题：未在公开信息中明确其: 解决问题 </span><br><span class="line">rschwabco/clip-vit-base-patch32-embeddings: 解决问题：图像和文本的特征嵌入提取 </span><br><span class="line">garyw/clinical-embeddings-300d-w2v-cr: 解决问题：生成临床文本的词向量表示 </span><br><span class="line">garyw/clinical-embeddings-600d-w2v-cr: 解决问题：处理临床文本向量化问题 </span><br><span class="line">garyw/clinical-embeddings-100d-ft-cr: 解决问题：提供临床文本的向量嵌入表示 </span><br><span class="line">[garyw/clinical-embeddings-300d-ft-cr]: 解决问题：提供临床文本的词嵌入表示 </span><br><span class="line">garyw/clinical-embeddings-600d-ft-cr: 解决问题：提供临床领域文本嵌入表示 </span><br><span class="line">garyw/clinical-embeddings-100d-w2v-oa-all: 解决问题：临床文本信息的向量表示问题 </span><br><span class="line">garyw/clinical-embeddings-300d-w2v-oa-all: 解决问题：处理临床文本信息的嵌入表示 </span><br><span class="line">[garyw/clinical-embeddings-100d-ft-oa-all]: 解决问题：[生成临床文本的词向量表示]</span><br><span class="line">garyw/clinical-embeddings-300d-ft-oa-all: 解决问题：生成临床文本的词向量表示 </span><br><span class="line">garyw/clinical-embeddings-100d-gl-cr: 解决问题：生成临床文本的词向量表示 </span><br><span class="line">garyw/clinical-embeddings-100d-gl-oa-all: 解决问题：生成临床文本的向量嵌入表示 </span><br><span class="line">[Banano/banano-sd-embeddings]: 解决问题：文本到图像生成的嵌入问题 </span><br><span class="line">johnsmith007/Embeddings: 解决问题：未找到该模型: 解决问题相关信息 </span><br><span class="line">garyw/clinical-embeddings-300d-gl-oa-all: 解决问题：生成临床文本的嵌入表示 </span><br><span class="line">garyw/clinical-embeddings-600d-gl-oa-all: 解决问题：临床文本信息的向量化表示 </span><br><span class="line">[garyw/clinical-embeddings-300d-gl-cr]: 解决问题：提供临床文本的嵌入表示 </span><br><span class="line">garyw/clinical-embeddings-600d-gl-cr: 解决问题：临床相关文本特征表示问题 </span><br><span class="line">rschwabco/facenet-embeddings: 解决问题：人脸特征提取与嵌入表示 </span><br><span class="line">deprem-ml/distilroberta-tweet-clustering-embeddings: 解决问题：对推文进行聚类嵌入处理 </span><br><span class="line">[rschwabco/clip-embeddings]: 解决问题：[图像与文本特征嵌入表示]</span><br><span class="line">nekofura/Embeddings: 解决问题：未找到该模型: 解决问题的相关信息。 </span><br><span class="line">[LithiumBC/embeddings]: 解决问题：文本向量化及特征表示问题 </span><br><span class="line">pix2pix-zero-library/direction_embeddings: 解决问题：图像转换方向的嵌入表示问题 </span><br><span class="line">[FoodDesert/Boring_Embeddings]: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">nolanaatama/embeddings: 解决问题：未找到该模型: 解决问题相关信息 </span><br><span class="line">Kizi-Art/embeddings: 解决问题：未找到相关信息，无法明确: 解决问题。 </span><br><span class="line">toto10/embeddings: 解决问题：未在公开信息中找到明确: 解决问题 </span><br><span class="line">[EarthnDusk/Isometric_Embeddings]: 解决问题：未找到该模型: 解决问题相关信息 </span><br><span class="line">furgo/hb_embeddings: 解决问题：暂未在公开信息明确其: 解决问题 </span><br><span class="line">AmornthepKladmee/embeddings: 解决问题：未找到该模型: 解决问题的相关信息。 </span><br><span class="line">ilahazs/embeddings: 解决问题：未找到该模型: 解决问题相关信息 </span><br><span class="line">Surteng/embeddings: 解决问题：未检索到该模型相关: 解决问题信息 </span><br><span class="line">Loug/embeddings: 解决问题：暂未找到该模型具体: 解决问题信息 </span><br><span class="line">9o99/embeddings: 解决问题：暂未获取到该模型: 解决问题信息 </span><br><span class="line">vivi2023/embeddings: 解决问题：未找到该模型在huggingface的相关信息，无法明确: 解决问题。 </span><br><span class="line">hc95qc/embeddings: 解决问题：未查询到该模型相关: 解决问题信息 </span><br><span class="line">Seltion/embeddings: 解决问题：未找到该模型在huggingface相关信息，无法确定: 解决问题。 </span><br><span class="line">[Upword/gpt-neox-20b-embeddings]: 解决问题：文本向量化及特征提取 </span><br><span class="line">SlavyanDesu/embeddings: 解决问题：未找到该模型具体: 解决问题相关信息 </span><br><span class="line">shalomma/llama-7b-embeddings: 解决问题：将文本转换为向量表示 </span><br><span class="line">wtmsb/embeddings: 解决问题：未找到该模型相关有效信息，无法确定。 </span><br><span class="line">[next-social/sd-embeddings]: 解决问题：文本到图像生成相关嵌入问题 </span><br><span class="line">johncerpa/clip-embeddings: 解决问题：图像与文本特征嵌入问题 </span><br><span class="line">[dkrugman/clip-embeddings]: 解决问题：图像和文本的嵌入表示问题 </span><br><span class="line">rimOPS/embeddings: 解决问题：暂未获取到该模型: 解决问题信息 </span><br><span class="line">trinitishop14045/clip-embeddings: 解决问题：图像与文本特征嵌入问题 </span><br><span class="line">[deerslab/llama-7b-embeddings]: 解决问题：将文本转换为向量表示 </span><br><span class="line">dhanyaXchandra/embeddings: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">[SenY/embeddings]: 解决问题：文本向量化以用于语义分析等 </span><br><span class="line">ahupeter/embeddings: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">srodan/pleroma-inversion-embeddings: 解决问题：暂未明确该模型具体: 解决问题 </span><br><span class="line">JoeHart/embeddings: 解决问题：暂未找到该模型: 解决问题的相关信息 </span><br><span class="line">TaiouIV/embeddings: 解决问题：暂未搜索到该模型: 解决问题的相关信息。 </span><br><span class="line">[luxluna/embeddings]: 解决问题：未找到该模型相关信息，无法明确: 解决问题。 </span><br><span class="line">RealestMeri/Embeddings: 解决问题：暂未找到该模型: 解决问题的相关信息。 </span><br><span class="line">[gmp-dev/gmp-embeddings]: 解决问题：生成文本嵌入向量用于下游任务 </span><br><span class="line">KanonDes/embeddings: 解决问题：未找到相关信息，无法明确: 解决问题。 </span><br><span class="line">ffxvs/embeddings-collection: 解决问题：未找到明确具体: 解决问题信息 </span><br><span class="line">lokCX/embeddings: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">meeagain212/embeddings: 解决问题：未找到该模型在huggingface上的有效信息，无法明确: 解决问题。 </span><br><span class="line">jomcs/embeddings: 解决问题：暂未找到该模型具体: 解决问题信息 </span><br><span class="line">Srio/embeddings: 解决问题：未在huggingface找到相关信息，无法确定。 </span><br><span class="line">[SquareHat/clip-embeddings]: 解决问题：图像和文本特征嵌入问题 </span><br><span class="line">RectalWorm/embeddings: 解决问题：暂未找到该模型相关有效: 解决问题信息 </span><br><span class="line">Lucky555/embeddings: 解决问题：未找到该模型相关用途信息 </span><br><span class="line">michaeleliot/clip-embeddings: 解决问题：图像和文本的特征嵌入问题 </span><br><span class="line">2phi/embeddings: 解决问题：将文本转换为向量表示</span><br><span class="line">jpohhhh/embeddings_from_msmarco-MiniLM-L-6-v3: 解决问题：信息检索与文本相似度计算 </span><br><span class="line">radames/blip_image_embeddings: 解决问题：将图像转换为向量嵌入表示 </span><br><span class="line">gunduverse/embeddings: 解决问题：未找到该模型在huggingface上相关: 解决问题信息。 </span><br><span class="line">TNitro/embeddings: 解决问题：暂未从公开信息获取到: 解决问题描述 </span><br><span class="line">ahsaputro/nolanaatama_embeddings: 解决问题：暂未找到该模型: 解决问题的相关信息。 </span><br><span class="line">pinapelz/embeddings: 解决问题：未找到该模型相关有效: 解决问题信息 </span><br><span class="line">HUNTERDEBASTADOR/embeddings: 解决问题：未在huggingface找到该模型相关有效信息，无法确定: 解决问题。 </span><br><span class="line">Yanderu/Embeddings: 解决问题：未在huggingface找到相关: 解决问题信息 </span><br><span class="line">yanuararif/embeddings: 解决问题：未在搜索中明确该模型: 解决问题 </span><br><span class="line">[HuynhTrong/embeddings_all_datasets_v4]: 解决问题：未在huggingface找到相关信息，无法明确。 </span><br><span class="line">[mlhub/embeddings]: 解决问题：未找到该模型相关有效信息，无法明确。 </span><br><span class="line">haodi50/embeddings: 解决问题：未找到相关信息，无法明确: 解决问题 </span><br><span class="line">licyk/sd-embeddings: 解决问题：用于Stable Diffusion的文本嵌入问题 </span><br><span class="line">AndreiBlahovici/sentence_embeddings: 解决问题：将句子转换为向量表示 </span><br><span class="line">Echiki/embeddings: 解决问题：未在huggingface找到该模型，无法明确: 解决问题。 </span><br><span class="line">stablediffusionapi/load_lora_embeddings: 解决问题：加载LoRA嵌入以增强SD模型效果 </span><br><span class="line">radames/segment-anything-embeddings-base: 解决问题：图像分割特征嵌入表示问题 </span><br><span class="line">radames/segment-anything-embeddings-large: 解决问题：图像分割特征嵌入提取 </span><br><span class="line">radames/segment-anything-embeddings-huge: 解决问题：图像分割特征嵌入问题 </span><br><span class="line">[skims/embeddings]: 解决问题：未找到该模型在huggingface上的相关信息，无法明确: 解决问题。 </span><br><span class="line">kazp/embeddings: 解决问题：暂未找到该模型在huggingface的有效信息，无法确定: 解决问题。 </span><br><span class="line">[chatbert/bert-base-cased-embeddings]: 解决问题：将文本转换为有意义向量表示 </span><br><span class="line">[chatbert/xlm-roberta-base-embeddings]: 解决问题：文本嵌入表示及语义理解问题 </span><br><span class="line">Ichiro0001/embeddings: 解决问题：暂未获取到该模型: 解决问题的相关信息。 </span><br><span class="line">joshuaKnauber/clip-embeddings: 解决问题：实现图像和文本的特征嵌入 </span><br><span class="line">ghostsniper7/embeddings: 解决问题：暂未找到该模型具体: 解决问题信息 </span><br><span class="line">Blaxzter/LaBSE-sentence-embeddings: 解决问题：多语言句子嵌入表示问题 </span><br><span class="line">seikohajimari/embeddings: 解决问题：未在huggingface查到相关: 解决问题信息 </span><br><span class="line">aimutation05/embeddings: 解决问题：未找到该模型在huggingface上的有效信息，无法确定: 解决问题。 </span><br><span class="line">botp/embeddings: 解决问题：暂未搜索到该模型具体: 解决问题信息 </span><br><span class="line">servbot/embeddings: 解决问题：未查询到该模型: 解决问题的相关信息。 </span><br><span class="line">uivid64/embeddings_sd: 解决问题：可能用于稳定扩散相关特征嵌入 </span><br><span class="line">[MaiKahoru/embeddingSlime]: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">[rahul1003/ELMo_Embeddings]: 解决问题：生成上下文相关词向量表示 </span><br><span class="line">axhello/embeddings: 解决问题：未查询到该模型相关: 解决问题信息 </span><br><span class="line">Renqf/embeddings: 解决问题：暂未找到该模型相关问题解决信息 </span><br><span class="line">OliAiart/embeddings: 解决问题：未搜索到该模型: 解决问题的明确信息 </span><br><span class="line">JCTN/embeddings: 解决问题：未找到有效信息，无法明确问题 </span><br><span class="line">zetavg/zh-tw-pythia-1b-a12k-f84566-embeddings-gcp-a100-trans-t3-d2ad: 解决问题：中文台湾地区文本嵌入问题 </span><br><span class="line">DividingSky/clip-embeddings: 解决问题：实现图像与文本的特征嵌入转换 </span><br><span class="line">zh-tw-llm-dv/zh-tw-pythia-6.9b-a12k-te01-embeddings-ea1: 解决问题：中文（台湾）文本向量化表示 </span><br><span class="line">cryptlord/embeddings: 解决问题：未找到该模型: 解决问题的相关信息 </span><br><span class="line">StanderPM/clips-embeddings: 解决问题：文本和图像的特征嵌入表示 </span><br><span class="line">brunovianna/laion-logo-embeddings: 解决问题：生成logo的嵌入表示 </span><br><span class="line">[brunovianna/AVA_image_clip_embeddings]: 解决问题：生成图像的CLIP嵌入表示 </span><br><span class="line">js101/clip-embeddings: 解决问题：图像与文本嵌入表示问题 </span><br><span class="line">[cundiff/clip-embeddings]: 解决问题：实现图像和文本的特征嵌入转换 </span><br><span class="line">zh-tw-llm-dv/zh-tw-llm-ta01-pythia-1b-ta8000-v1-a_1_embeddings-a100-t02-3d435e: 解决问题：未找到该模型: 解决问题的明确信息。 </span><br><span class="line">[uretan/clip-embeddings]: 解决问题：图像和文本的特征嵌入表示</span><br><span class="line">zh-tw-llm-dv/zh-tw-llm-ta01-pythia-1b-ta8000-v1-b_1_embeddings_and_attention-a100-t02-713b8e: 解决问题：暂未找到该模型: 解决问题的相关信息 </span><br><span class="line">zh-tw-llm-dv/zh-tw-llm-ta01-pythia-6.9b-ta8000-v1-a_1_embeddings-h100-t01-c5daa1: 解决问题：暂未找到该模型: 解决问题的相关信息。 </span><br><span class="line">igorsantana/rnn-embeddings-songs: 解决问题：生成歌曲的RNN嵌入表示 </span><br><span class="line">zh-tw-llm-dv/zh-tw-llm-ta01-pythia-6.9b-ta8000-v1-a_1_embeddings-h100-t01-c5daa1-8bit: 解决问题：未找到相关信息，无法明确: 解决问题 </span><br><span class="line">zetavg/zh-tw-llm-ta01-pythia-6.9b-ta8000-v1-a_1_embeddings-h100-t01-c5daa1-8bit-2: 解决问题：暂未获取到该模型具体: 解决问题信息 </span><br><span class="line">Word2vec/polyglot_words_embeddings_afr: 解决问题：将南非荷兰语单词转换为向量 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>关于图相关的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">florentgbelidji/blip_image_embeddings: 解决问题：将图像转换为向量嵌入表示 </span><br><span class="line">crumb/ViT-L-14-Token-Embeddings: 解决问题：图像视觉特征的嵌入表示问题 </span><br><span class="line">ProGamerGov/knollingcase-embeddings-sd-v2-0: 解决问题：生成特定风格图像嵌入表示 </span><br><span class="line">KnutJaegersberg/sentence_transformer_mpnet_v2_to_kg_embeddings_wikidata5m: 解决问题：将句子转换为知识图谱嵌入</span><br><span class="line">ProGamerGov/winter-cat-embeddings-sd-v2-1: 解决问题：用于StableDiffusion生成特定风格图像 </span><br><span class="line">Priyanshu9991/sd2.1_768_embeddings: 解决问题：生成StableDiffusion 2.1图像嵌入 </span><br><span class="line">rschwabco/clip-vit-base-patch32-embeddings: 解决问题：图像和文本的特征嵌入提取 </span><br><span class="line">[Banano/banano-sd-embeddings]: 解决问题：文本到图像生成的嵌入问题 </span><br><span class="line">[rschwabco/clip-embeddings]: 解决问题：[图像与文本特征嵌入表示]</span><br><span class="line">pix2pix-zero-library/direction_embeddings: 解决问题：图像转换方向的嵌入表示问题 </span><br><span class="line">[next-social/sd-embeddings]: 解决问题：文本到图像生成相关嵌入问题 </span><br><span class="line">johncerpa/clip-embeddings: 解决问题：图像与文本特征嵌入问题 </span><br><span class="line">[dkrugman/clip-embeddings]: 解决问题：图像和文本的嵌入表示问题 </span><br><span class="line">trinitishop14045/clip-embeddings: 解决问题：图像与文本特征嵌入问题 </span><br><span class="line">[SquareHat/clip-embeddings]: 解决问题：图像和文本特征嵌入问题 </span><br><span class="line">michaeleliot/clip-embeddings: 解决问题：图像和文本的特征嵌入问题 </span><br><span class="line">radames/blip_image_embeddings: 解决问题：将图像转换为向量嵌入表示 </span><br><span class="line">radames/segment-anything-embeddings-base: 解决问题：图像分割特征嵌入表示问题 </span><br><span class="line">radames/segment-anything-embeddings-large: 解决问题：图像分割特征嵌入提取 </span><br><span class="line">radames/segment-anything-embeddings-huge: 解决问题：图像分割特征嵌入问题 </span><br><span class="line">joshuaKnauber/clip-embeddings: 解决问题：实现图像和文本的特征嵌入 </span><br><span class="line">DividingSky/clip-embeddings: 解决问题：实现图像与文本的特征嵌入转换 </span><br><span class="line">StanderPM/clips-embeddings: 解决问题：文本和图像的特征嵌入表示 </span><br><span class="line">[brunovianna/AVA_image_clip_embeddings]: 解决问题：生成图像的CLIP嵌入表示 </span><br><span class="line">js101/clip-embeddings: 解决问题：图像与文本嵌入表示问题 </span><br><span class="line">[cundiff/clip-embeddings]: 解决问题：实现图像和文本的特征嵌入转换 </span><br><span class="line">[uretan/clip-embeddings]: 解决问题：图像和文本的特征嵌入表示</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250625112125-daebff51.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250625112125-daebff51.html" itemprop="url">LangChain构建 Chroma</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-06-25T11:21:25+08:00">
                2025-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>先要安装依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain-chroma</span><br></pre></td></tr></table></figure>

<p>Chroma可以以多种模式运行。以下是每种模式的示例，均与LangChain集成：</p>
<ul>
<li>in-memory-在Python脚本或Jupyter笔记本中</li>
<li>in-memory with persistance-在脚本或笔记本中保存/加载到磁盘</li>
<li>in a docker container-作为在本地机器或云中运行的服务器</li>
</ul>
<p>与任何其他数据库一样，您可以进行以下操作：</p>
<ul>
<li>add</li>
<li>get</li>
<li>update</li>
<li>upsert</li>
<li>delete</li>
<li>peek</li>
</ul>
<p>而.query则运行相以性搜索。查看完整文档，请访问docs。要直接访问这些方法，可以使用，_collection.method()。</p>
<p>基本示例在这个基本示例中，我们获取《乔布斯演讲稿》，将其分割成片段，使用开源嵌入模型进行嵌入，加载到Chroma中，然后进行查询。</p>
<p>乔布斯演讲知识库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"># knowledge.txt</span><br><span class="line"></span><br><span class="line">Thank you. I&#x27;m honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college and this is the closest I&#x27;ve ever gotten to a college graduation.</span><br><span class="line">谢谢大家。很荣幸能和你们，来自世界最好大学之一的毕业生们，一块儿参加毕业典礼。老实说，我大学没有毕业，今天恐怕是我一生中离大学毕业最近的一次了。</span><br><span class="line"></span><br><span class="line">Today I want to tell you three stories from my life. That&#x27;s it. No big deal. Just three stories.</span><br><span class="line">今天我想告诉大家来自我生活的三个故事。没什么大不了的，只是三个故事而已。</span><br><span class="line"></span><br><span class="line">The first story is about connecting the dots.</span><br><span class="line">第一个故事，如何串连生命中的点滴。</span><br><span class="line"></span><br><span class="line">I dropped out of Reed College after the first six months but then stayed around as a drop-in for another 18 months or so before I really quit. So why did I drop out? It started before I was born. My biological mother was a young, unwed graduate student, and she decided to put me up for adoption. She felt very strongly that I should be adopted by college graduates, so everything was all set for me to be adopted at birth by a lawyer and his wife, except that when I popped out, they decided at the last minute that they really wanted a girl. So my parents, who were on a waiting list, got a call in the middle of the night asking, &quot;We&#x27;ve got an unexpected baby boy. Do you want him?&quot; They said, &quot;Of course.&quot; My biological mother found out later that my mother had never graduated from college and that my father had never graduated from high school. She refused to sign the final adoption papers. She only relented a few months later when my parents promised that I would go to college.</span><br><span class="line">我在里得大学读了六个月就退学了，但是在18个月之后--我真正退学之前，我还常去学校。为何我要选择退学呢？这还得从我出生之前说起。我的生母是一个年轻、未婚的大学毕业生，她决定让别人收养我。她有一个很强烈的信仰，认为我应该被一个大学毕业生家庭收养。于是，一对律师夫妇说好了要领养我，然而最后一秒钟，他们改变了主意，决定要个女孩儿。然后我排在收养人名单中的养父母在一个深夜接到电话，“很意外，我们多了一个男婴，你们要吗？”“当然要！”但是我的生母后来又发现我的养母没有大学毕业，养父连高中都没有毕业。她拒绝在领养书上签字。几个月后，我的养父母保证会让我上大学，她妥协了。</span><br><span class="line"></span><br><span class="line">This was the start in my life. And 17 years later, I did go to college, but I naively chose a college that was almost as expensive as Stanford, and all of my working-class parents&#x27; savings were being spent on my college tuition. After six months, I couldn&#x27;t see the value in it. I had no idea what I wanted to do with my life, and no idea of how college was going to help me figure it out, and here I was, spending all the money my parents had saved their entire life. So I decided to drop out and trust that it would all work out OK. It was pretty scary at the time, but looking back, it was one of the best decisions I ever made. The minute I dropped out, I could stop taking the required classes that didn&#x27;t interest me and begin dropping in on the ones that looked far more interesting.</span><br><span class="line">这是我生命的开端。十七年后，我上大学了，但是我很无知地选了一所差不多和斯坦福一样贵的学校，几乎花掉我那蓝领阶层养父母一生的积蓄。六个月后，我觉得不值得。我看不出自己以后要做什么，也不晓得大学会怎样帮我指点迷津，而我却在花销父母一生的积蓄。所以我决定退学，并且相信没有做错。一开始非常吓人，但回忆起来，这却是我一生中作的最好的决定之一。从我退学的那一刻起，我可以停止一切不感兴趣的必修课，开始旁听那些有意思得多的课。</span><br><span class="line"></span><br><span class="line">It wasn&#x27;t all romantic. I didn&#x27;t have a dorm room, so I slept on the floor in friends&#x27; rooms. I returned Coke bottles for the five-cent deposits to buy food with, and I would walk the seven miles across town every Sunday night to get one good meal a week at the Hare Krishna temple. I loved it. And much of what I stumbled into by following my curiosity and intuition turned out to be priceless later on. Let me give you one example.</span><br><span class="line">事情并不那么美好。我没有宿舍可住，睡在朋友房间的地上。为了吃饭，我收集五分一个的旧可乐瓶，每个星期天晚上步行七英里到哈尔-克里什纳庙里改善一下一周的伙食。我喜欢这种生活方式。能够遵循自己的好奇和直觉前行后来被证明是多么的珍贵。让我来给你们举个例子吧。</span><br><span class="line"></span><br><span class="line">Reed College at that time offered perhaps the best calligraphy instruction in the country. Throughout the campus every poster, every label on every drawer was beautifully hand-calligraphed. Because I had dropped out and didn&#x27;t have to take the normal classes, I decided to take a calligraphy class to learn how to do this. I learned about serif and sans-serif typefaces, about varying the amount of space between different letter combinations, about what makes great typography great. It was beautiful, historical, artistically subtle in a way that science can&#x27;t capture, and I found it fascinating.</span><br><span class="line">当时的里德大学提供可能是全国最好的书法指导。校园中每一张海报，抽屉上的每一张标签，都是漂亮的手写体。由于我已退学，不用修那些必修课，我决定选一门书法课上上。在这门课上，我学会了“serif”和&quot;sans-serif&quot;两种字体、学会了怎样在不同的字母组合中改变字间距、学会了怎样写出好的字来。这是一种科学无法捕捉的微妙，楚楚动人、充满历史底蕴和艺术性，我觉得自己被完全吸引了。</span><br><span class="line"></span><br><span class="line">None of this had even a hope of any practical application in my life. But ten years later when we were designing the first Macintosh computer, it all came back to me, and we designed it all into the Mac. It was the first computer with beautiful typography. If I had never dropped in on that single course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts, and since Windows just copied the Mac, it&#x27;s likely that no personal computer would have them.</span><br><span class="line">当时我并不指望书法在以后的生活中能有什么实用价值。但是，十年之后，我们在设计第一台 Macintosh计算机时，它一下子浮现在我眼前。于是，我们把这些东西全都设计进了计算机中。这是第一台有这么漂亮的文字版式的计算机。要不是我当初在大学里偶然选了这么一门课，Macintosh计算机绝不会有那么多种印刷字体或间距安排合理的字号。要不是Windows照搬了 Macintosh，个人电脑可能不会有这些字体和字号。</span><br><span class="line"></span><br><span class="line">If I had never dropped out, I would have never dropped in on that calligraphy class and personals computers might not have the wonderful typography that they do.</span><br><span class="line">要不是退了学，我决不会碰巧选了这门书法课，个人电脑也可能不会有现在这些漂亮的版式了。</span><br><span class="line"></span><br><span class="line">Of course it was impossible to connect the dots looking forward when I was in college, but it was very, very clear looking backwards ten years later. Again, you can&#x27;t connect the dots looking forward. You can only connect them looking backwards, so you have to trust that the dots will somehow connect in your future. You have to trust in something--your gut, destiny, life, karma, whatever--because believing that the dots will connect down the road will give you the confidence to follow your heart, even when it leads you off the well-worn path, and that will make all the difference.</span><br><span class="line">当然，我在大学里不可能从这一点上看到它与将来的关系。十年之后再回头看，两者之间关系就非常、非常清楚了。你们同样不可能从现在这个点上看到将来；只有回头看时，才会发现它们之间的关系。所以你必须相信，那些点点滴滴，会在你未来的生命里，以某种方式串联起来。你必须相信一些东西——你的勇气、宿命、生活、因缘，随便什么——因为相信这些点滴能够一路连接会给你带来循从本觉的自信，它使你远离平凡，变得与众不同。</span><br><span class="line"></span><br><span class="line">My second story is about love and loss. I was lucky. I found what I loved to do early in life. Woz and I started Apple in my parents&#x27; garage when I was 20. We worked hard and in ten years, Apple had grown from just the two of us in a garage into a $2 billion company with over 4,000 employees. We&#x27;d just released our finest creation, the Macintosh, a year earlier, and I&#x27;d just turned 30, and then I got fired. How can you get fired from a company you started? Well, as Apple grew, we hired someone who I thought was very talented to run the company with me, and for the first year or so, things went well. But then our visions of the future began to diverge, and eventually we had a falling out. When we did, our board of directors sided with him, and so at 30, I was out, and very publicly out. What had been the focus of my entire adult life was gone, and it was devastating. I really didn&#x27;t know what to do for a few months. I felt that I had let the previous generation of entrepreneurs down, that I had dropped the baton as it was being passed to me. I met with David Packard and Bob Noyce and tried to apologize for screwing up so badly. I was a very public failure and I even thought about running away from the Valley. But something slowly began to dawn on me. I still loved what I did. The turn of events at Apple had not changed that one bit. I&#x27;d been rejected but I was still in love. And so I decided to start over.</span><br><span class="line">第二个故事是关于爱与失的。我很幸运，很早就发现自己喜欢做的事情。我二十岁的时候就和沃茨在父母的车库里开创了苹果公司。我们工作得很努力，十年后，苹果公司成长为拥有四千名员工，价值二十亿的大公司。我们刚刚推出了最好的创意，Macintosh操作系统，在这之前的一年，也就是我刚过三十岁，我被解雇了。你怎么可能被一个亲手创立的公司解雇？事情是这样的，在公司成长期间，我雇佣了一个我们认为非常聪明，可以和我一起经营公司的人。一年后，我们对公司未来的看法产生分歧，董事会站在了他的一边。于是，在我三十岁的时候，我出局了，很公开地出局了。我整个成年生活的焦点没了，这很要命。一开始的几个月我真的不知道该干什么。我觉得我让公司的前一代创建者们失望了，我把传给我的权杖给弄丢了。我与戴维德·帕珂德和鲍勃·诺埃斯见面，试图为这彻头彻尾的失败道歉。我败得如此之惨以至于我想要逃离硅谷。但有个东西在慢慢地叫醒我：我还爱着我从事的行业。这次失败一点儿都没有改变这一点。我被逐了，但我仍爱着我的事业。我决定重新开始。</span><br><span class="line"></span><br><span class="line">I didn&#x27;t see it then, but it turned out that getting fired from Apple was the best thing that could have ever happened to me. The heaviness of being successful was replaced by the lightness of being a beginner again, less sure about everything. It freed me to enter one of the most creative periods in my life. During the next five years I started a company named NeXT, another company named Pixar and fell in love with an amazing woman who would become my wife. Pixar went on to create the world&#x27;s first computer-animated feature film, &quot;Toy Story,&quot; and is now the most successful animation studio in the world.</span><br><span class="line">当时我没有看出来，但事实证明“被苹果开除”是发生在我身上最好的事。成功的重担被重新起步的轻松替代，对任何事情都不再特别看重，这让我感觉如此自由，进入一生中最有创造力的阶段。接下来的五年，我创立了一个叫NeXT的公司，接着又建立了Pixar，然后与后来成为我妻子的女人相爱。Pixar出品了世界第一个电脑动画电影：“玩具总动员”，现在它已经是世界最成功的动画制作工作室了。</span><br><span class="line"></span><br><span class="line">In a remarkable turn of events, Apple bought NeXT and I returned to Apple and the technology we developed at NeXT is at the heart of Apple&#x27;s current renaissance, and Lorene and I have a wonderful family together.</span><br><span class="line">在一系列的成功运转后，苹果收购了NeXT，我又回到了苹果。我们在NeXT开发的技术在苹果的复兴中起了核心作用，另外劳琳和我组建了一个幸福的家庭。</span><br><span class="line"></span><br><span class="line">I&#x27;m pretty sure none of this would have happened if I hadn&#x27;t been fired from Apple. It was awful-tasting medicine but I guess the patient needed it. Sometimes life&#x27;s going to hit you in the head with a brick. Don&#x27;t lose faith. I&#x27;m convinced that the only thing that kept me going was that I loved what I did. You&#x27;ve got to find what you love, and that is as true for work as it is for your lovers. Your work is going to fill a large part of your life, and the only way to be truly satisfied is to do what you believe is great work, and the only way to do great work is to love what you do. If you haven&#x27;t found it yet, keep looking, and don&#x27;t settle. As with all matters of the heart, you&#x27;ll know when you find it, and like any great relationship it just gets better and better as the years roll on. So keep looking. Don&#x27;t settle.</span><br><span class="line">我非常确信，如果我没有被苹果炒掉，这些就都不会发生。这个药的味道太糟了，但是我想病人需要它。有些时候，生活会给你迎头一棒。不要丧失信心。我确信唯一让我一路走下来的是我对自己所做事情的热爱。你必须去找你热爱的东西，对工作如此，对你的爱人也是这样的。工作会占据你生命中很大的一部分，你只有相信自己做的是伟大的工作，你才能怡然自得。如果你还没有找到，那么就继续找，不要停。全心全意地找，当你找到时，你会知道的。就像任何真诚的关系，随着时间的流逝，只会越来越紧密。所以继续找，不要停。</span><br><span class="line"></span><br><span class="line">My third story is about death. When I was 17 I read a quote that went something like &quot;If you live each day as if it was your last, someday you&#x27;ll most certainly be right.&quot; It made an impression on me, and since then, for the past 33 years, I have looked in the mirror every morning and asked myself, &quot;If today were the last day of my life, would I want to do what I am about to do today?&quot; And whenever the answer has been &quot;no&quot; for too many days in a row, I know I need to change something. Remembering that I&#x27;ll be dead soon is the most important thing I&#x27;ve ever encountered to help me make the big choices in life, because almost everything--all external expectations, all pride, all fear of embarrassment or failure--these things just fall away in the face of death, leaving only what is truly important. Remembering that you are going to die is the best way I know to avoid the trap of thinking you have something to lose. You are already naked. There is no reason not to follow your heart.</span><br><span class="line">我的第三个故事关于死亡。我17岁的时候读到过一句话“如果你把每一天都当作最后一天过，有一天你会发现你是正确的”。这句话给我留下了深刻的印象。从那以后，过去的33年，每天早上我都会对着镜子问自己：“如果今天是我的最后一天，我会不会做我想做的事情呢？”如果连着一段时间，答案都是否定的的话，我就知道我需要改变一些东西了。提醒自己就要死了是我遇见的最大的帮助，帮我作了生命中的大决定。因为几乎任何事——所有的荣耀、骄傲、对难堪和失败的恐惧——在死亡面前都会消隐，留下真正重要的东西。提醒自己就要死亡是我知道的最好的方法，用来避开担心失去某些东西的陷阱。你已经赤裸裸了，没有理由不听从于自己的心愿。</span><br><span class="line"></span><br><span class="line">About a year ago, I was diagnosed with cancer. I had a scan at 7:30 in the morning and it clearly showed a tumor on my pancreas. I didn&#x27;t even know what a pancreas was. The doctors told me this was almost certainly a type of cancer that is incurable, and that I should expect to live no longer than three to six months. My doctor advised me to go home and get my affairs in order, which is doctors&#x27; code for &quot;prepare to die.&quot; It means to try and tell your kids everything you thought you&#x27;d have the next ten years to tell them, in just a few months. It means to make sure that everything is buttoned up so that it will be as easy as possible for your family. It means to say your goodbyes.</span><br><span class="line">大约一年前，我被诊断出患了癌症。我早上七点半作了扫描，清楚地显示在我的胰腺有一个肿瘤。我当时都不知道胰腺是什么东西。医生们告诉我这几乎是无法治愈的，我还有三到六个月的时间。我的医生建议我回家，整理一切。在医生的辞典中，这就是“准备死亡”的意思。就是意味着把要对你小孩说十年的话在几个月内说完；意味着把所有东西搞定，尽量让你的家庭活得轻松一点；意味着你要说“永别”了。</span><br><span class="line"></span><br><span class="line">I lived with that diagnosis all day. Later that evening I had a biopsy where they stuck an endoscope down my throat, through my stomach into my intestines, put a needle into my pancreas and got a few cells from the tumor. I was sedated but my wife, who was there, told me that when they viewed the cells under a microscope, the doctor started crying, because it turned out to be a very rare form of pancreatic cancer that is curable with surgery. I had the surgery and, thankfully, I am fine now.</span><br><span class="line">我整日都想着那诊断书的事情。后来有天晚上我做了一个活切片检查，他们将一个内窥镜伸进我的喉咙，穿过胃，到达肠道，用一根针在我的胰腺肿瘤上取了几个细胞。我当时是被麻醉的，但是我的妻子告诉我，那些医生在显微镜下看到细胞的时候开始尖叫，因为发现这竟然是一种非常罕见的可用手术治愈的胰腺癌症。我做了手术，现在，我痊愈了。</span><br><span class="line"></span><br><span class="line">This was the closest I&#x27;ve been to facing death, and I hope it&#x27;s the closest I get for a few more decades. Having lived through it, I can now say this to you with a bit more certainty than when death was a useful but purely intellectual concept. No one wants to die, even people who want to go to Heaven don&#x27;t want to die to get there, and yet, death is the destination we all share. No one has ever escaped it. And that is as it should be, because death is very likely the single best invention of life. It&#x27;s life&#x27;s change agent; it clears out the old to make way for the new. right now, the new is you. But someday, not too long from now, you will gradually become the old and be cleared away. Sorry to be so dramatic, but it&#x27;s quite true. Your time is limited, so don&#x27;t waste it living someone else&#x27;s life. Don&#x27;t be trapped by dogma, which is living with the results of other people&#x27;s thinking. Don&#x27;t let the noise of others&#x27; opinions drown out your own inner voice, and most important, have the courage to follow heart and intuition. They somehow already know what you truly want to become. Everything else is secondary.</span><br><span class="line">这是我最接近死亡的时候，我也希望是我未来几十年里最接近死亡的一次。这次死里逃生让我比以往只知道死亡是一个有用而纯粹书面概念的时候更确信地告诉你们，没有人愿意死，即使那些想上天堂的人们也不愿意通过死亡来达到他们的目的。但是死亡是每个人共同的终点，没有人能够逃脱。也应该如此，因为死亡很可能是生命最好的发明。它去陈让新。现在，你们就是“新”。但是有一天，不用太久，你们有会慢慢变老然后死去。抱歉，这很戏剧性，但却是真的。你们的时间是有限的，不要浪费在重复别人的生活上。不要被教条束缚，那意味着会和别人思考的结果一块儿生活。不要被其他人的喧嚣观点掩盖自己内心真正的声音。你的直觉和内心知道你想要变成什么样子。所有其他东西都是次要的。</span><br><span class="line"></span><br><span class="line">When I was young, there was an amazing publication called _The Whole Earth Catalogue_, which was one of the bibles of my generation. It was created by a fellow named Stuart Brand not far from here in Menlo Park, and he brought it to life with his poetic touch. This was in the late 1960s, before personal computers and desktop publishing, so it was all made with typewriters, scissors, and Polaroid cameras. it was sort of like Google in paperback form 35 years before Google came along. It was idealistic, overflowing with neat tools and great notions. Stuart and his team put out several issues of the _The Whole Earth Catalogue_, and then when it had run its course, they put out a final issue. It was the mid-1970s and I was your age. On the back cover of their final issue was a photograph of an early morning country road, the kind you might find yourself hitch-hiking on if you were so adventurous. Beneath were the words, &quot;Stay hungry, stay foolish.&quot; It was their farewell message as they signed off. &quot;Stay hungry, stay foolish.&quot; And I have always wished that for myself, and now, as you graduate to begin anew, I wish that for you. Stay hungry, stay foolish.</span><br><span class="line">我年轻的时候，有一份叫做《完整地球目录》的好杂志，是我们这一代人的圣经之一。它是一个叫斯纠华特·布兰的、住在离这不远的曼罗公园的家伙创立的。他用诗一般的触觉将这份杂志带到世界。那是六十年代后期，个人电脑出现之前，所以这份杂志全是用打字机、剪刀和偏光镜制作的。有点像软皮包装的google，不过却早了三十五年。它理想主义，全文充斥着灵巧的工具和伟大的想法。斯纠华特和他的小组出版了几期“完整地球目录”，在完成使命之前，他们出版了最后一期。那是七十年代中期，我和你们差不多大。最后一期的封底是一张清晨乡村小路的照片，如果你有冒险精神，可以自己找到这条路。下面有一句话，“保持饥饿，保持愚蠢”。这是他们的告别语，“保持饥饿，保持愚蠢”。我常以此勉励自己。现在，在你们即将踏上新旅程的时候，我也希望你们能这样。保持饥饿，保持愚蠢。</span><br><span class="line"></span><br><span class="line">Thank you all, very much.</span><br><span class="line">非常感谢。</span><br></pre></td></tr></table></figure>

<p>下面是一个完整的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#示例：chroma_base.py</span></span><br><span class="line"><span class="comment"># pip install langchain</span></span><br><span class="line"><span class="comment"># pip install langchain-community</span></span><br><span class="line"><span class="comment"># pip install langchain-chroma</span></span><br><span class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="comment"># pip install -U langchain-huggingface</span></span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"><span class="comment">#加载文指并将其分割成片段</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;../knowledge.txt&quot;</span>,encoding=<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"><span class="comment">#将其分割成片段</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1500</span>,chunk_overlap=<span class="number">0</span>)</span><br><span class="line">docs = text_splitter.split_documents(documents)</span><br><span class="line"><span class="comment">#创建开源嵌入函数</span></span><br><span class="line">embedding_function = HuggingFaceEmbeddings(model_name=<span class="string">&quot;all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line"><span class="comment">#将其加载到Chroma中</span></span><br><span class="line">db = Chroma.from_documents(docs,embedding_function)</span><br><span class="line"><span class="comment">#进行查询</span></span><br><span class="line">query = <span class="string">&quot;Pixar公司是做什么的？&quot;</span></span><br><span class="line">docs = db.similarity_search(query)</span><br><span class="line"><span class="comment">#打印结果</span></span><br><span class="line"><span class="built_in">print</span>(docs[<span class="number">0</span>].page_content)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>如果我们希望将向量持久化到磁盘可以按照下面方式写</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install langchain-chroma</span></span><br><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="comment">#  pip install -U langchain-huggingface</span></span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">persistent_client = chromadb.PersistentClient()</span><br><span class="line"><span class="comment">#创建开源族入丽数</span></span><br><span class="line">embedding_function = HuggingFaceEmbeddings(model_name=<span class="string">&quot;all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line">collection = persistent_client.get_or_create_collection(<span class="string">&quot;collection_name&quot;</span>)</span><br><span class="line">collection.add(ids=[<span class="string">&quot;1&quot;</span>,<span class="string">&quot;2&quot;</span>,<span class="string">&quot;3&quot;</span>],documents=[<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>])</span><br><span class="line">langchain_chroma = Chroma(</span><br><span class="line">    client=persistent_client,</span><br><span class="line">    collection_name=<span class="string">&quot;collection_name&quot;</span>,</span><br><span class="line">    embedding_function=embedding_function,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;在年合中有&quot;</span>, langchain_chroma._collection.count(),<span class="string">&quot;个文档&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="CRUD"><a href="#CRUD" class="headerlink" title="CRUD"></a>CRUD</h2><p>下面是一个简单的增删改查可以体会一下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="comment"># pip install langchain-chroma</span></span><br><span class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="comment"># pip install langchain-text-splitters</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_huggingface <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"></span><br><span class="line">loader = TextLoader(<span class="string">&quot;/content/onedrive/colab/knowledge.txt&quot;</span>,encoding=<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"><span class="comment"># #将其分割成片段</span></span><br><span class="line">text_splitter = CharacterTextSplitter(chunk_size=<span class="number">1500</span>,chunk_overlap=<span class="number">0</span>)</span><br><span class="line">docs = text_splitter.split_documents(documents)</span><br><span class="line"><span class="comment"># #创建开源嵌入函数</span></span><br><span class="line">embedding_function = HuggingFaceEmbeddings(model_name=<span class="string">&quot;all-MiniLM-L6-v2&quot;</span>)</span><br><span class="line">query=<span class="string">&quot;Pixar公司是做什么的？&quot;</span></span><br><span class="line"><span class="comment"># #创建简单的ids</span></span><br><span class="line">ids = [<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(docs)+<span class="number">1</span>)]</span><br><span class="line"><span class="comment">#添加数据</span></span><br><span class="line">example_db = Chroma.from_documents(docs,embedding_function,ids=ids)</span><br><span class="line">docs = example_db.similarity_search(query)</span><br><span class="line"><span class="comment">#更新文档的元数据</span></span><br><span class="line">docs[<span class="number">0</span>].metadata = &#123;</span><br><span class="line">	<span class="string">&quot;source&quot;</span>:<span class="string">&quot;/content/onedrive/colab/knowledge.txt&quot;</span>,</span><br><span class="line">	<span class="string">&quot;new_value&quot;</span>:<span class="string">&quot;hello world&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;更所前内容：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(example_db._collection.get(ids=[ids[<span class="number">0</span>]]))</span><br><span class="line">example_db.update_document(ids[<span class="number">0</span>],docs [<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;更新后内容：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(example_db._collection.get(ids=[ids[<span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;删除前计数&quot;</span>,example_db._collection.count())</span><br><span class="line"><span class="built_in">print</span>(example_db._collection.get(ids=[ids[-<span class="number">1</span>]]))</span><br><span class="line">example_db._collection.delete(ids=[ids[-<span class="number">1</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;删除后计数&quot;</span>,example_db._collection.count())</span><br><span class="line"><span class="built_in">print</span>(example_db._collection.get(ids=[ids[-<span class="number">1</span>]]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="使用openAI"><a href="#使用openAI" class="headerlink" title="使用openAI"></a>使用openAI</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#示例：chroma._openai.py</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="comment"># pip install langchain-chroma</span></span><br><span class="line"><span class="keyword">from</span> langchain_chroma <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">import</span> chromadb</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextsplitter</span><br><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">persistent_client = chromadb.Persistentclient()</span><br><span class="line">new_client = chromadb.Ephemeralclient()</span><br><span class="line"><span class="comment">#加载文档并将其分剖成片段</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;../../resource/knowledge.txt&quot;</span>,encoding=<span class="string">&quot;UTF-8&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br><span class="line"><span class="comment">#将其分割成片段</span></span><br><span class="line">text_splitter = CharacterTextsplitter(chunk_size=<span class="number">1500</span>,chunk_overlap=<span class="number">0</span>)</span><br><span class="line">docs = text_splitter.split_documents(documents)</span><br><span class="line">openai_lc_client = Chroma.from_documents(</span><br><span class="line">    docs,embeddings,client=new_client,collection_name=<span class="string">&quot;openai_collection&quot;</span></span><br><span class="line">)</span><br><span class="line">query=<span class="string">&quot;Pixar公司是做什么的？&quot;</span></span><br><span class="line">docs = openai_lc_client.similarity_search(query)</span><br><span class="line"><span class="built_in">print</span>(docs[<span class="number">0</span>].page_content)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250625011750-3eadec25.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250625011750-3eadec25.html" itemprop="url">Embedding与向量数据库</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-06-25T01:17:50+08:00">
                2025-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="向量数据库"><a href="#向量数据库" class="headerlink" title="向量数据库"></a>向量数据库</h2><p>向量数据库(Vector Database),也叫矢量数据库，主要用来存储和处理向量数据。在数学中，向量是有大小和方向的量，可以使用带箭头的线段表示，箭头指向即为向量的方向，线段的长度表示向量的大小。两个向量的距离或者相似性可以通过欧式距离或者余弦距离得到。图像、文本和音视频这种非结构化数据都可以通过某种变换或者嵌入学习转化为向量数据存储到向量数据库中，从而实现对图像、文本和音视频的相似性搜索和检索。这意味着您可以使用向量数据库根据语义或上下文含义查找最相似或相关的数据，而不是使用基于精确匹配或预定义标准查询数据库的传统方法。向量数据库的主要特点是高效存储与检索。利用索引技术和向量检索算法能实现高维大数据下的快速响应。向量数据库也是一种数据库，除了要管理向量数据外，还是支持对传统结构化数据的管理。实际使用时，有很多场景会同时对向量字段和结构化字段进行过滤检索，这对向量数据库来说也是一种挑战</p>
<h2 id="向量嵌入"><a href="#向量嵌入" class="headerlink" title="向量嵌入"></a>向量嵌入</h2><p>对于传统数据库，搜索功能都是基于不同的索引方式(BTree、倒排索引等)加上精确匹配和排序算法(BM25、TF-DF)等实现的。本质还是基于文本的精确匹配，这种索引和搜索算法对于关键字的搜索功能非常合适，但对于语义搜索功能就非常弱。</p>
<p>例如，如果你搜索“小狗”，那么你只能得到带有“小狗”关键字相关的结果，而无法得到“柯基”“金毛”等结果，因为“小狗”和“金毛”是不同的词，传统数据库无法识别它们的语义关系，所以传统的应用需要人为的将“小狗”和“金毛”等词之间打上特征标签进行关联，这样才能实现语义搜索。而如何将生成和挑选特征这个过程，也被称为Feature Engineering(特征工程)，它是将原始数据转化成更好的表达问题本质的特征的过程。</p>
<p>但是如果你需要处理非结构化的数据，就会发现非结构化数据的特征数量会开始快速膨胀，例如我们处理的是图像、音频、视频等数据，这个过程就变得非常困难。例如，对于图像，可以标注颜色、形状、纹理、边缘、对象、场景等特征，但是这些特征太多了，而且很难人为的进行标注，所以我们需要一种自动化的方式来提取这些特征，而这可以通过Vector Embedding实现。</p>
<p>Vector Embedding是由AI模型（例如大型语言模型LLM)生成的，它会根据不同的算法生成高维度的向量数据，代表着数据的不同特征，这些特征代表了数据的不同维度。例如，对于文本，这些特征可能包括词汇、语法、语义、情感、情绪、主题、上下文等。对于音频，这些特征可能包括音调、节奏、音高、音色、音量、语音、音乐等。</p>
<p>例如对于目前来说，文本向量可以通过OpenAl的text-embedding-ada-O02模型生成，图像向量可以通过clip-vit-base-patch32模型生成，而音频向量可以通过wav2vec2-base-960h模型生成。这些向量都是通过AI模型生成的，所以它们都是具有语义信息的。</p>
<p>例如我们将这句话“Your text string goes here”用text-embedding-ada-002模型进行文本Embedding,它会生成一个1536维的向量，得到的结果是这样：“-0.006929283495992422，-0.005336422007530928,·,-454713225452536e-05,-0,024047505110502243”,它是一个长度为1536的数组。这个向量就包含了这句话的所有特征，这些特征包括词汇、语法，我们可以将它存入向量数据库中，以便我们后续进行语义搜索。</p>
<h2 id="特征和向量"><a href="#特征和向量" class="headerlink" title="特征和向量"></a>特征和向量</h2><p>虽然向量数据库的核心在于相似性搜索(Similarity Search),但在深入了解相似性搜索前，我们需要先详细了解一下特征和向量的概念和原理。</p>
<p>我们先思考一个问题？为什么我们在生活中区分不同的物品和事物？</p>
<p>如果从理论角度出发，这是因为我门会通过源别不同事物之间不同的特征来识别种类，例如分别不同种类的小狗，就可以适过体型大小.毛发长层、旱子长短等烤证来区分。如下面这张照片按照体型序，可以看到体型拉入的狗靠近坐标抽右边，这样就能得到-一个体型特征的一维坐和对应的数值，从0到1的数字中得到每只狗在坐标系中的位置。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad4bc87031.png" alt="image.png"></p>
<p>然而单靠一个体型大小的特征并不够，像照片中哈士奇、金毛和拉布拉多的体型就非常接近，我们无法区分。所以我们会继续观察其它的特征，例如毛发的长短。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad4d4820bf.png" alt="image.png"></p>
<p>这样每只狗对应一个二维坐标点，我们就能轻易的将哈士奇、金毛和拉布拉多区分开来，如果这时仍然无法很好的区分德牧和罗威纳犬。我们就可以继续再从其它的特征区分，比如鼻子的长短，这样就能得到个三维的坐标系和每只狗在三维坐标系中的位置。<br>在这种情况下，只要特征足够多，就能够将所有的狗区分开来，最后就能得到一个高维的坐标系，虽然我们想象不出高维坐标系长什么样，但是在数组中，我们只需要一直向数组中追加数字就可以了。<br>实际上，只要维度够多，我们就能够将所有的事物区分开来，世间万物都可以用一个多维坐标系来表示，它们都在一个高维的特征空间中对应着一个坐标点。<br>那这和相似性搜索(Similarity Search)有什么关系呢？你会发现在上面的二维坐标中，德牧和罗威纳犬的坐标就非常接近，这就意味着它们的特征也非常接近。我们都知道向量是具有大小和方向的数学结构，所以可以将这些特征用向量来表示，这样就能够通过计算向量之间的距离来判断它们的相似度，这就是相似性测量</p>
<h2 id="相似性测量-Similarity-Measurement"><a href="#相似性测量-Similarity-Measurement" class="headerlink" title="相似性测量(Similarity Measurement)"></a>相似性测量(Similarity Measurement)</h2><p>上面我们讨论了向量数据库的不同搜索算法，但是还没有讨论如何衡量相似性。在相似性搜索中，需要计算两个向量之间的距离，然后根据距离来判断它们的相似度。而如何计算向量在高维空间的距离呢？有三种常见的向量相似度算法：欧几里德距离、余弦相似度和点积相似度。</p>
<h2 id="欧几里得距离-Euclidean-Distance"><a href="#欧几里得距离-Euclidean-Distance" class="headerlink" title="欧几里得距离(Euclidean Distance)"></a>欧几里得距离(Euclidean Distance)</h2><p>欧几里得距离是用于衡量两个点（或向量）之间的直线距离的一种方法。它是最常用的距离度量之一，尤其在几何和向量空间中。可以将其理解为两点之间的“直线距离”。</p>
<p>其中，A和B分别表示两个向量，n表示向量的维度。</p>
<h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>对于两个向量A=(a1,a2,·,an)和B=(b1,b2,·,bn),它们之间的欧几里得距离可以计算为：</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad55558e9d.png" alt="image.png"></p>
<p>对应的图片为</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad56573fdf.png" alt="image.png"></p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad5d10d556.png" alt="image.png"></p>
<p>欧几里得距离算法的优点是可以反映向量的绝对距离，适用于需要考虑向量长度的相似性计算。例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度。</p>
<h2 id="余弦相似度-Cosine-Similarity"><a href="#余弦相似度-Cosine-Similarity" class="headerlink" title="余弦相似度(Cosine Similarity)"></a>余弦相似度(Cosine Similarity)</h2><p>余弦相似度(Cosine Similarity)是一种用于衡量两个向量之间相似度的指标。它通过计算两个向量夹角<br>的余弦值来判断它们的相似程度。其值介于-1和1之间，通常用于文本分析和推荐系统等领域：</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad5ff583ce.png" alt="image.png"></p>
<p>A·B是向量的点积<br>||A||和‖B||是向量的范数（即向量的长度）<br>计算步骤</p>
<ol>
<li>点积计算：计算两个向量的点积，即各个对应元素相乘再求和。</li>
<li>范数计算：计算每个向量的范数，通常是欧几里得范数，即每个元素平方和的平方根。</li>
<li>代入公式：将点积和范数代入公式，得到余弦相似度。</li>
</ol>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad6378ab4e.png" alt="image.png"></p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>值域：结果在-1到1之间。1表示完全相似，0表示不相关，-1表示完全相反。<br>余弦相似度对向量的长度不敏感，只关注向量的方向，因此适用于高维向量的相似性计算。例如语义搜索和文档分类。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad65be8e96.png" alt="image.png"></p>
<h2 id="相似性搜索-Similarity-Search"><a href="#相似性搜索-Similarity-Search" class="headerlink" title="相似性搜索(Similarity Search)"></a>相似性搜索(Similarity Search)</h2><p>既然我们知道了可以通过比较向量之间的距离来判断它们的相似度，那么如何将它应用到真实的场景中呢？如果想要在一个海量的数据中找到和某个向量最相似的向量，我们需要对数据库中的每个向量进行一次比较计算，但这样的计算量是非常巨大的，所以我们需要一种高效的算法来解决这个问题。高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：</p>
<p>1)减少向量大小一通过降维或减少表示向量值的长度。<br>2)缩小搜索范围一可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围仅在最接近的簇中进行，或者通过最相似的分支进行过滤。</p>
<p>我们首先来介绍一下大部分算法共有的核心概念，也就是聚类。</p>
<h3 id="K-Means"><a href="#K-Means" class="headerlink" title="K-Means"></a>K-Means</h3><p>我们可以在保存向量数据后，先对向量数据先进行聚类。例如下图在二维坐标系中，划定了4个聚类中心，然后将每个向量分配到最近的聚类中心，经过聚类算法不断调整聚类中心位置，这样就可以将向量数据分成4个簇。每次搜索时，只需要先判断搜索向量属于哪个簇，然后再在这一个簇中进行搜索，这样就从4个簇的搜索范围减少到了1个簇，大大减少了搜索的范围。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad69f7f776.png" alt="image.png"></p>
<p>工作原理<br>1.初始化：选择k个随机点作为初始聚类中心。<br>2.分配：将每个向量分配给离它最近的聚类中心，形成k个簇。<br>3.更新：重新计算每个簇的聚类中心，即簇内所有向量的平均值。<br>4.迭代：重复“分配”和“更新”步骤，直到聚类中心不再变化或达到最大迭代次数。</p>
<p>举例说明<br>假设我们有一组二维向量（点），例如：「(1,2)，(2,1)，（4,5)，(5,4)，（8,9)川，我们希望将它们聚成四个簇（<br>k=4)。<br>1.初始化：随机选择四个点作为初始聚类中心，比如(1,2)、(2,1)、(4,5)和(8,9)。<br>2.分配：计算每个点到四个聚类中心的距离，将每个点分配给最近的聚类中心。<br>形成四个簇：[(1,2)]，[(2,1)]，[(4,5)，(5,4)]，[(8,9)]</p>
<ul>
<li>点(1,2)更接近(1,3)</li>
<li>点(2,1)更接近(2,1)</li>
<li>点（4,5)更接近（4,5)</li>
<li>点(5,4)更接近（4,5)</li>
<li>点(8,9)更接近(8,9)</li>
</ul>
<p>3.更新：计算每个簇的新聚类中心</p>
<ul>
<li>第一个簇的聚类中心是(1,2)</li>
<li>第二个簇的聚类中心是(2,1)</li>
<li>第三个簇的新聚类中心是(4+5)/2，(5+4)/2)=(4.5,4.5)</li>
<li>第四个簇的聚类中心是(8,9)</li>
</ul>
<p>4.迭代：重复分配和更新步骤，直到聚类中心不再变化。</p>
<p>优点：通过将向量聚类，我们可以在进行相似性搜索时只需计算查询向量与每个聚类中心之间的距离，而不是与所有向量计算距离。这大大减少了计算量，提高了搜索效率。这种方法在处大规慎数据时尤其有效，可以显著加快搜索速度。</p>
<p>缺点，例如在搜索的时候，如果搜索的内容正好处于两个分类区域的中间，就很有可能遗漏掉最相似的向量。</p>
<h2 id="Hierarchical-Navigable-Small-Worlds-HNSW"><a href="#Hierarchical-Navigable-Small-Worlds-HNSW" class="headerlink" title="Hierarchical Navigable Small Worlds (HNSW)"></a>Hierarchical Navigable Small Worlds (HNSW)</h2><p>除了聚类以外，也可以通过构建树或者构建图的方式来实现近似最近邻搜索。这种方法的基本思想是每次将向量加到数据库中的时候，就先找到点它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻捷豪和最短路径计算，直到找到最相似的向量。这种算法能保证搜索的质量，但是如果图中所以的节点都以最短的路径相连，如图中最下面的一层，那么在搜索的时候，就同样需要遍历所有的节点。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad758a2ba8.png" alt="image.png"></p>
<p>解决这个问题的思路与常见的跳表算法相以，如下图要搜索跳表，从最高层开始，沿着具有最长“跳过的边向右移动。如果发现当前节点的值大于要搜索的值-我们知道已经超过了目标，因此我们会在下一级中向前一个节点。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad776355a6.png" alt="image.png"></p>
<p>通过一个简单的例子来说明HNSW的工作原理。<br>假设我们有以下五个二维向量：</p>
<ul>
<li>A=(1,2)</li>
<li>B=(2,3)</li>
<li>C=(3,4)</li>
<li>D=(8,9</li>
<li>E=(9,10)<br>我们要使用HNSW来找到与查询向量Q=(2,2)最相似的向量。</li>
</ul>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad86fbb70a.png" alt="image.png"></p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad84ea7e34.png" alt="image.png"></p>
<p>总结: 通过这种层次化的搜索过程，HNSW能够快速缩小搜索范围，在大规模数据中高效找到近以最近邻。</p>
<h1 id="基于Embedding的问答助手和意图匹配"><a href="#基于Embedding的问答助手和意图匹配" class="headerlink" title="基于Embedding的问答助手和意图匹配"></a>基于Embedding的问答助手和意图匹配</h1><h2 id="Embedding-models-嵌入模型"><a href="#Embedding-models-嵌入模型" class="headerlink" title="Embedding models(嵌入模型)"></a>Embedding models(嵌入模型)</h2><p>Embeddings类是一个专为与文本嵌入模型进行交互而设计的类。有许多嵌入模型提供商（如OpenAl.Cohere、Hugging Face等)-这个类旨在为它们提供一个标准接口。</p>
<p>Embeddings类会为文本创建一个向量表示。这很有用，因为这意味着我们可以在向量空间中思考文本，并做一些类似语义搜索的事情，比如在向量空间中寻找最相似的文本片段。</p>
<p>LangChain中的基本Embeddings类提供了两种方法：一个用于嵌入文档，另一个用于嵌入查询。前者，embed_documents接受多个文本作为输入，而后者.embed_query接受单个文本。之所以将它们作为两个单独的方法，是因为一些嵌入提供商对文档（要搜索的文档）和查询（搜索查询本身）有不同的嵌入方法。</p>
<p>.embed_.query将返回一个浮点数列表，而.embed._documents将返回一个浮点数列表的列表。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad8c23f7ce.png" alt="image.png"></p>
<p>设置OpenAl<br>首先，我们需要安装OpenAl合作伙伴包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain-openai</span><br></pre></td></tr></table></figure>

<p>访问AP需要一个API密钥，您可以通过创建帐户并转到这里来获取它。一旦我们有了密钥，我们希望通过运行以下命令将其设置为环境变量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export OPENAI_API KEY=&quot;...&quot;</span><br></pre></td></tr></table></figure>

<p>如果您不想设置环境变量，可以在初始化OpenAl LLM类时通过api_key命名参数直接传递密钥：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line">embeddings_model OpenAIEmbeddings(api_key=<span class="string">&quot;...&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>否则，您可以不带任何参数初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line">embeddings_model OpenAIEmbeddings()</span><br></pre></td></tr></table></figure>

<p>完整的Case</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># pip install langchain-openai</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line">embeddings_model = OpenAIEmbeddings()</span><br><span class="line">embeddings = embeddings_model.embed_documents(</span><br><span class="line">	[</span><br><span class="line">		<span class="string">&quot;嗨！&quot;</span>,</span><br><span class="line">		<span class="string">&quot;哦，你好！&quot;</span>,</span><br><span class="line">		<span class="string">&quot;你叫什么名字？&quot;</span>,</span><br><span class="line">		<span class="string">&quot;我的朋及们叫我World&quot;</span>,</span><br><span class="line">		<span class="string">&quot;Hello World！&quot;</span></span><br><span class="line">	]</span><br><span class="line">)</span><br><span class="line"><span class="comment">#第一个打印是embeddings/的文本数量，第二个打印是第一段文本的embed中ing向量维度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(embeddings),<span class="built_in">len</span>(embeddings[<span class="number">0</span>]))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="https://pic.yihao.de/pic/2025/06/25/685ad976dc0fa.png" alt="image.png"></p>
<h2 id="嵌入单个查询-embed-query"><a href="#嵌入单个查询-embed-query" class="headerlink" title="嵌入单个查询(embed query)"></a>嵌入单个查询(embed query)</h2><p>使用.embed_query来嵌入单个文本片段（例如，用于与其他嵌入的文本片段进行t比较）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install langchain-openai</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line">embeddings_model = OpenAIEmbeddings()</span><br><span class="line">embeddings = embeddings_model.embed_documents(</span><br><span class="line">	[</span><br><span class="line">		<span class="string">&quot;嗨！&quot;</span>,</span><br><span class="line">		<span class="string">&quot;哦，你好！&quot;</span>,</span><br><span class="line">		<span class="string">&quot;你叫什么名字？&quot;</span>,</span><br><span class="line">		<span class="string">&quot;我的朋及们叫我World&quot;</span>,</span><br><span class="line">		<span class="string">&quot;Hello World！&quot;</span></span><br><span class="line">	]</span><br><span class="line">)</span><br><span class="line">embedded_query=embeddings_model.embed_query(<span class="string">&quot;对话中提到的名字是什么？&quot;</span>)</span><br><span class="line"><span class="comment">#查询嵌入的前五个值</span></span><br><span class="line"><span class="built_in">print</span>(embedded_query[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[0.603292866277936101,-0.689463472291827282,0.03418809175491333,-0.8811715596774592996,...]</span><br></pre></td></tr></table></figure>

<h2 id="问答助手"><a href="#问答助手" class="headerlink" title="问答助手"></a>问答助手</h2><p>从知识库中，找到一个和问题最接近的类似问题。我的知识库集合为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&quot;OpenAI的ChatGPT,是一个强大的语言模型。&quot;,</span><br><span class="line">&quot;天空是蓝色的,阳光灿烂。&quot;,</span><br><span class="line">&quot;人工智能正在改变世界。&quot;,</span><br><span class="line">&quot;Python是一种流行的编程语言。&quot;</span><br></pre></td></tr></table></figure>

<p>完整Case</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="comment">#pip install numpy</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> dot</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm</span><br><span class="line">client = OpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义调用Embedding API的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding</span>(<span class="params">text</span>):</span><br><span class="line">    response = client.embeddings.create(</span><br><span class="line">        <span class="built_in">input</span>=text,</span><br><span class="line">        model=<span class="string">&quot;text-embedding-ada-002&quot;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> response.data[<span class="number">0</span>].embedding</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1,vec2</span>):</span><br><span class="line"><span class="comment">#计算并返回两个向量之间的余弦相似度，公式为：两个向量的点积除以它们范数的乘积。</span></span><br><span class="line">    <span class="keyword">return</span> dot(vec1 , vec2) / (norm(vec1) * norm(vec2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数search_documents,该函数接受一个查询字符串query和一个文档列表documents作为输入。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search_documents</span>(<span class="params">query,documents</span>):</span><br><span class="line">    <span class="comment">#调用get_embedding函数生成查询字符串的嵌入向量query_embedding。</span></span><br><span class="line">    query_embedding = get_embedding(query)</span><br><span class="line">    <span class="comment">#对每个文档调用get_embedding函数生成文档的嵌入向量，存储在document_embeddings列表中。</span></span><br><span class="line">    document_embeddings = [get_embedding(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> documents]</span><br><span class="line">    <span class="comment">#计算查询嵌入向量与每个文档嵌入向量之间的余弦相似度，存储在similarities列表中</span></span><br><span class="line">    similarities = [cosine_similarity(query_embedding,doc_embedding) <span class="keyword">for</span> doc_embedding <span class="keyword">in</span> document_embeddings]</span><br><span class="line">    <span class="comment">#找到相似度最高的文档的索引most_similar_index。</span></span><br><span class="line">    most_similar_index = similarities.index(<span class="built_in">max</span>(similarities))</span><br><span class="line">    <span class="comment">#返回相似度最高的文档和相似度得分。</span></span><br><span class="line">    <span class="keyword">return</span> documents[most_similar_index],<span class="built_in">max</span>(similarities)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#测试文本搜索功能</span></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    documents = [</span><br><span class="line">        <span class="string">&quot;OpenAI的ChatGPT,是一个强大的语言模型。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;天空是蓝色的,阳光灿烂。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;人工智能正在改变世界。&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Python是一种流行的编程语言。&quot;</span></span><br><span class="line">    ]</span><br><span class="line">    query=<span class="string">&quot;天空是什么颜色的？&quot;</span></span><br><span class="line"></span><br><span class="line">    most_similar_document,similarity_score = search_documents(query,documents)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;最相似的文档：<span class="subst">&#123;most_similar_document&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;相似性得分：<span class="subst">&#123;similarity_score&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250623172846-6ac773fb.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250623172846-6ac773fb.html" itemprop="url">Dify构建智能体</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-06-23T17:28:46+08:00">
                2025-06-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这里部署社区版本dify，可以参考这个文档 <a target="_blank" rel="noopener" href="https://docs.dify.ai/en/getting-started/install-self-hosted/docker-compose">Deploy with Docker Compose - Dify Docs</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/langgenius/dify.git --branch 0.15.8</span><br><span class="line">cd /volume1/docker/dify/docker</span><br><span class="line">cp .env.example .env</span><br><span class="line"></span><br><span class="line"># 求nginx暴漏端口</span><br><span class="line">EXPOSE_NGINX_PORT=8083</span><br><span class="line">EXPOSE_NGINX_SSL_PORT=4143</span><br><span class="line"></span><br><span class="line"># 提前创建好目录</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/redis/data</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/db/dat</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/db/data</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/weaviate</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/db/data</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/app/storage</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/certbot/conf</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/certbot/www</span><br><span class="line">mkdir -p /volume1/docker/dify/docker/volumes/certbot/conf/live</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line"></span><br><span class="line">docker-compose -p dify up -d</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>访问 localhost:8083/install 等待安装</p>
<p>设置好管理员账号即可正常使用localhost:8083/apps</p>
<h1 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h1><p>我们以翻译机器人为例演示工作流的使用。</p>
<p>创建一个空白应用</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/68591ff28382d.png" alt="image-20250623173544724"></p>
<p>填写相应信息</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/68592020d1f8b.png" alt="image-20250623173631087"></p>
<p>构造简单工作流整体链路</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/6859203bc975a.png" alt="image-20250623173658904"></p>
<p>在开始节点新增 [input]和[lang]变量，用来接受用户的要翻译的文本，和要翻译成的目标语言。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/68592073d3f39.png" alt="image-20250623173755291"></p>
<p>设置LLM大模型，需要注意的是SYSTEM prompt可以点击✨动态生成，引用上游变量可以用大括号{}。USER prompt填写用户输入的Input内容即可。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/6859209314965.png" alt="image-20250623173825862"></p>
<p>结束时将大模型内容输出出来。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/68592114e7c41.png" alt="image-20250623174036300"></p>
<h1 id="Agent智能体构建"><a href="#Agent智能体构建" class="headerlink" title="Agent智能体构建"></a>Agent智能体构建</h1><p>智能体中可以包含多个工作流，下面需要将上面的工作流嵌入到Agent智能体中。</p>
<p><img src="https://pic.yihao.de/pic/2025/06/23/68592163129cf.png" alt="image-20250623174153810"></p>
<p>后续点击发布即可</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250424191420-7995bdd5.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250424191420-7995bdd5.html" itemprop="url">量化示意图</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-04-24T19:14:20+08:00">
                2025-04-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://pic.yihao.de/pic/2025/04/24/680a1d23e46f7.png" alt="image-20250424191440341"></p>
<p><img src="https://pic.yihao.de/pic/2025/04/24/680a1d3f1ebab.png" alt="image-20250424191506927"></p>
<p><img src="https://pic.yihao.de/pic/2025/04/24/680a1d5a2e041.png" alt="image-20250424191534518"></p>
<p><img src="https://pic.yihao.de/pic/2025/04/24/680a1d68a6e67.png" alt="image-20250424191549179"></p>
<p><img src="https://pic.yihao.de/pic/2025/04/24/680a1d7557fd0.png" alt="image-20250424191601872"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250414092156-d83062e5.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250414092156-d83062e5.html" itemprop="url">架构设计-行情</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-04-14T09:21:56+08:00">
                2025-04-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>借鉴LSM-Tree的设计思路，先将数据写入缓存池，在周期性写入到CSV</p>
<p><img src="https://pic.yihao.de/pic/2025/04/14/67fc633a32868.png" alt="image-20250414092200176"></p>
<p>写入策略: </p>
<ol>
<li>当内存满&gt;n时，将内容溢写入磁盘</li>
<li>当时间满&gt;s时，将内存溢写入磁盘</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250312154507-800f254c.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250312154507-800f254c.html" itemprop="url">白漂个大厂商的大模型产品</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-03-12T15:45:07+08:00">
                2025-03-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这里我们以硅基流动为例，演示白漂路径。</p>
<p>Cherry Studio提供统一的API入口，不管你是用DeepSeek，还是用OpenAI，这里提供统一的操作入口。下载地址<a target="_blank" rel="noopener" href="https://nas.yihao.de/sharing/QqHS6F0nY">https://nas.yihao.de/sharing/QqHS6F0nY</a></p>
<h1 id="获取API密钥"><a href="#获取API密钥" class="headerlink" title="获取API密钥"></a>获取API密钥</h1><p>安装好APP，到【模型服务】这里，可以看到【API密钥】下面有一个【点击这里获取密钥】点击</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d100b277405.png"></p>
<p>到硅基流动官网注册一个帐号，到API 密钥复制，粘贴到上图空白处</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d133189506d.png" alt="image-20250312150907203"></p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>到助手，简单测试一下，能否直接使用，找到Qwen 2.5-7B-Instruct ，发送消息即可。</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d133af275a9.png" alt="image-20250312151139469"></p>
<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p>模型决定了问答的质量，市面上有非常多的模型，HuggingFace是模型开源库，里面提供大量的开源模型<a target="_blank" rel="noopener" href="https://huggingface.co/">https://huggingface.co/</a></p>
<h1 id="白漂模型"><a href="#白漂模型" class="headerlink" title="白漂模型"></a>白漂模型</h1><p>要用就用白漂模型，也是够用了。进入硅基流动官网</p>
<p>在【模型广场】-&gt; 【筛选】-&gt; 【只看免费】，找到对应的模型名称，这里以【deepseek-ai/DeepSeek-R1-Distill-Qwen-7B】为例子</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d13431a06da.png" alt="image-20250312151347958"></p>
<p>到APP中，模型服务-&gt;硅基流动-&gt;管理 ， 粘贴对应的名称并搜索，点击➕添加</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d134be12b37.png" alt="image-20250312151609166"></p>
<p>之后到助手中，问答的时候选择添加的模型即可</p>
<p><img src="https://pic.yihao.de/pic/2025/03/12/67d1351132604.png" alt="image-20250312151732672"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/20250307125429-982f2e59.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hoey笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/posts/20250307125429-982f2e59.html" itemprop="url">地铁</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2025-03-07T12:54:29+08:00">
                2025-03-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="https://pic.yihao.de/pic/2025/03/07/67ca7b5622883.png" alt="dt"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">&gt;</a>
  </nav>



          </div>
          


          

<script src="https://giscus.app/client.js"
        data-repo="hoey194/note"
        data-repo-id="R_kgDOImfwSg"
        data-category="Announcements"
        data-category-id="DIC_kwDOImfwSs4CikdG"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">58</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hoey</span>

  
</div>














<span id="timeDate"></span>
<span id="times"></span>



<script>
  var now = new Date();
  function createtime() {
    var grt= new Date("01/01/2022 12:00:00");//此处修改你的建站时间或者网站上线时间
    now.setTime(now.getTime()+250);
    days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
    hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
    if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
    mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
    seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
    snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
    document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
    document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
  }
  setInterval("createtime()",250);
</script>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
